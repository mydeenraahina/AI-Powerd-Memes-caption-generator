{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e5df6e-835d-4cd3-ba4c-8c582c9c5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Concatenate, Reshape\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image \n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "df=pd.read_excel(\"Meme_Data_set.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe09d6d-779a-4d11-a851-29f574b6e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the  dataset:\n",
      "                                            Captions  \\\n",
      "0  my mom when i play with my phone for 30 minute...   \n",
      "1  Doing your own research for a test Copy and pa...   \n",
      "2  Making original memes Following the idea of an...   \n",
      "3  Upvote begging Upvoting your own meme on diffe...   \n",
      "4                     Food delivery person Superhero   \n",
      "\n",
      "                          Image URL                 Labels  \n",
      "0  https://i.imgflip.com/30b1gx.jpg  Mom, Phone, Instagram  \n",
      "1  https://i.imgflip.com/1ur9b0.jpg    Research, Wikipedia  \n",
      "2  https://i.imgflip.com/24y43o.jpg       Original, Repost  \n",
      "3  https://i.imgflip.com/22bdq6.jpg                 Upvote  \n",
      "4    https://i.imgflip.com/9ehk.jpg    Delivery, Superhero  \n",
      "Last few rows of the  dataset:\n",
      "                                             Captions  \\\n",
      "44                                        Normal meme   \n",
      "45  have a two tomoe sharingan having a eternal ma...   \n",
      "46                          Schools Cancelled Student   \n",
      "47                                runescape tunescape   \n",
      "48  a week of making a special meme making a meme ...   \n",
      "\n",
      "                           Image URL        Labels  \n",
      "44    https://i.imgflip.com/25w3.jpg          Meme  \n",
      "45   https://i.imgflip.com/govs4.jpg     Sharingan  \n",
      "46  https://i.imgflip.com/320xfw.jpg        School  \n",
      "47    https://i.imgflip.com/tau4.jpg     RuneScape  \n",
      "48    https://i.imgflip.com/46rh.jpg  Special Meme  \n",
      "\n",
      "Basic statistics of the  dataset:\n",
      "                                                 Captions  \\\n",
      "count                                                  49   \n",
      "unique                                                 49   \n",
      "top     my mom when i play with my phone for 30 minute...   \n",
      "freq                                                    1   \n",
      "\n",
      "                               Image URL               Labels  \n",
      "count                                 49                   49  \n",
      "unique                                49                   43  \n",
      "top     https://i.imgflip.com/30b1gx.jpg  Money, Toilet Paper  \n",
      "freq                                   1                    3  \n",
      "\n",
      "Information about the  dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49 entries, 0 to 48\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Captions   49 non-null     object\n",
      " 1   Image URL  49 non-null     object\n",
      " 2   Labels     49 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Exploratory Data Analysis (EDA) Performance\n",
    "# Display the first few rows of the  dataset\n",
    "print(\"First few rows of the  dataset:\")\n",
    "print(df.head()) \n",
    "\n",
    "# Display the first few rows of the testing data\n",
    "print(\"Last few rows of the  dataset:\")\n",
    "print(df.tail())  \n",
    "# Display basic statistics of the  dataset\n",
    "print(\"\\nBasic statistics of the  dataset:\")\n",
    "print(df.describe()) \n",
    "# Display information about the  dataset\n",
    "print(\"\\nInformation about the  dataset:\")\n",
    "print(df.info()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82a1570-3dac-4805-b511-1d485b5149c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "class data_cleaning:\n",
    "    #function to remove duplicates\n",
    "    def remove_duplicates(self,data):        \n",
    "        data.drop_duplicates(inplace=True)\n",
    "    #function to handle missing values\n",
    "    def handling_empty_cells(self,data):\n",
    "        data.dropna(inplace=True)\n",
    "obj=data_cleaning()\n",
    "obj.remove_duplicates(df)\n",
    "obj.handling_empty_cells(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1fdf4c-6f24-4b87-b137-0be286616a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def load_image(image_url):\\n    try:\\n        # Download image from URL\\n        response = requests.get(image_url)\\n        img = Image.open(BytesIO(response.content))\\n        img = img.convert(\\'RGB\\')  # Ensure image is in RGB mode\\n\\n        # Resize image to target size\\n        img = img.resize((224, 224))\\n\\n        # Convert image to array and preprocess for VGG16\\n        img = img_to_array(img)\\n        img = np.expand_dims(img, axis=0)\\n        img = preprocess_input(img)\\n\\n        return img\\n\\n    except Exception as e:\\n        print(f\"Error loading image from {image_url}: {str(e)}\")\\n        return None\\n\\n# Preprocess all images in the DataFrame\\nprocessed_images = []\\n\\nfor image_url in df[\\'Image URL\\']:\\n    processed_image = load_image(image_url)\\n    if processed_image is not None:\\n        processed_images.append(processed_image)\\n\\n# Convert processed_images to a numpy array\\nX_images = np.vstack(processed_images)\\n\\nprint(f\"Shape of preprocessed images: {X_images.shape}\")\\nimage_urls = df[\\'Image URL\\'].tolist()\\nX_images = np.zeros((len(image_urls), 4096))  # assuming VGG16 output shape for fc2 layer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def load_image(image_url):\n",
    "    try:\n",
    "        # Download image from URL\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = img.convert('RGB')  # Ensure image is in RGB mode\n",
    "\n",
    "        # Resize image to target size\n",
    "        img = img.resize((224, 224))\n",
    "\n",
    "        # Convert image to array and preprocess for VGG16\n",
    "        img = img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = preprocess_input(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {image_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Preprocess all images in the DataFrame\n",
    "processed_images = []\n",
    "\n",
    "for image_url in df['Image URL']:\n",
    "    processed_image = load_image(image_url)\n",
    "    if processed_image is not None:\n",
    "        processed_images.append(processed_image)\n",
    "\n",
    "# Convert processed_images to a numpy array\n",
    "X_images = np.vstack(processed_images)\n",
    "\n",
    "print(f\"Shape of preprocessed images: {X_images.shape}\")\n",
    "image_urls = df['Image URL'].tolist()\n",
    "X_images = np.zeros((len(image_urls), 4096))  # assuming VGG16 output shape for fc2 layer\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73eb0336-eb86-4e7c-9357-01c43ed0a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 18s/step\n",
      "Shape of extracted features: (49, 25088)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def load_image(image_url):\n",
    "    try:\n",
    "        # Download image from URL\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = img.convert('RGB')  # Ensure image is in RGB mode\n",
    "\n",
    "        # Resize image to target size expected by VGG16\n",
    "        img = img.resize((224, 224))\n",
    "        \n",
    "        # Convert image to array and preprocess for VGG16\n",
    "        img = img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = preprocess_input(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {image_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_vgg16_features(image_urls):\n",
    "    processed_images = []\n",
    "\n",
    "    for image_url in image_urls:\n",
    "        processed_image = load_image(image_url)\n",
    "        if processed_image is not None:\n",
    "            processed_images.append(processed_image)\n",
    "\n",
    "    X_images = np.vstack(processed_images)\n",
    "    \n",
    "    # Load VGG16 model without the top (fully connected layers)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Extract features using VGG16\n",
    "    features = base_model.predict(X_images)\n",
    "\n",
    "    # Flatten features to (number_of_images, 4096)\n",
    "    features = features.reshape(len(processed_images), -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Assuming df['Image URL'] contains your image URLs\n",
    "X_images = extract_vgg16_features(df['Image URL'].tolist())\n",
    "\n",
    "# Verify the shape of X_images after feature extraction\n",
    "print(f\"Shape of extracted features: {X_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59efcaa5-6449-453b-bf6d-39b613e74d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Captions:\n",
      "['my', 'mom', 'when', 'i', 'play', 'with', 'my', 'phone', 'for', '30', 'minutes', 'my', 'mom', 'when', 'she', 'looks', 'at', 'instagram', 'phone', 'for', 'two', 'hours']\n",
      "['doing', 'your', 'own', 'research', 'for', 'a', 'test', 'copy', 'and', 'pasting', 'from', 'wikipedia']\n",
      "['making', 'original', 'memes', 'following', 'the', 'idea', 'of', 'an', 'idiot', 'repost']\n",
      "\n",
      "Labels:\n",
      "['Mom', 'Phone', 'Instagram']\n",
      "['Research', 'Wikipedia']\n",
      "['Original', 'Repost']\n",
      "Tokenizer Vocabulary:\n",
      "{'the': 1, 'my': 2, 'a': 3, 'for': 4, 'when': 5, 'to': 6, 'i': 7, 'meme': 8, 'in': 9, 'with': 10, 'of': 11, 'school': 12, 'mom': 13, 'two': 14, 'and': 15, 'me': 16, 'play': 17, 'at': 18, 'from': 19, 'making': 20, 'upvoting': 21, 'on': 22, 'getting': 23, 'tell': 24, 'games': 25, 'how': 26, 'after': 27, 'corona': 28, 'have': 29, 'money': 30, 'toilet': 31, 'paper': 32, 'phone': 33, 'your': 34, 'own': 35, 'memes': 36, 'banned': 37, 'roblox': 38, 'saying': 39, 'surprise': 40, 'song': 41, 'minecraft': 42, 'social': 43, 'you': 44, 'feel': 45, 'face': 46, 'swimming': 47, 'instructor': 48, 'help': 49, 'hear': 50, 'is': 51, 'out': 52, 'one': 53, 'distancing': 54, 'normal': 55, 'family': 56, 'pizza': 57, 'everyone': 58, 'hard': 59, 'finally': 60, 'father': 61, 'news': 62, 'sharingan': 63, '30': 64, 'minutes': 65, 'she': 66, 'looks': 67, 'instagram': 68, 'hours': 69, 'doing': 70, 'research': 71, 'test': 72, 'copy': 73, 'pasting': 74, 'wikipedia': 75, 'original': 76, 'following': 77, 'idea': 78, 'an': 79, 'idiot': 80, 'repost': 81, 'upvote': 82, 'begging': 83, 'different': 84, 'accounts': 85, 'food': 86, 'delivery': 87, 'person': 88, 'superhero': 89, 'life': 90, 'marching': 91, 'band': 92, 'andom': 93, 'hashtags': 94, 'flamingoalbert': 95, 'yes': 96, 'kidnapping': 97, 'adoption': 98, 'small': 99, 'potatoes': 100, 'lite': 101, 'carbs': 102, 'not': 103, 'sing': 104, 'through': 105, 'full': 106, 'fix': 107, 'mistakes': 108, \"we're\": 109, 'done': 110, 'stop': 111, 'every': 112, 'four': 113, 'measures': 114, 'correct': 115, 'minor': 116, 'hiccups': 117, 'fans': 118, 'distance': 119, 'donate': 120, 'covid': 121, 'relief': 122, 'make': 123, 'dance': 124, 'boost': 125, 'numbers': 126, 'flex': 127, 'house': 128, 'music': 129, 'vi': 130, 'go': 131, 'outside': 132, 'video': 133, 'watching': 134, 'super': 135, 'bowl': 136, 'starting': 137, 'war': 138, 'dad': 139, 'enters': 140, 'room': 141, 'seeing': 142, 'report': 143, 'card': 144, 'calls': 145, 'him': 146, 'back': 147, 'civilian': 148, 'navy': 149, 'seal': 150, 'daddy': 151, 'mommy': 152, 'sight': 153, 'singing': 154, 'choralography': 155, 'burger': 156, 'king': 157, 'booger': 158, 'fling': 159, \"dadn't\": 160, 'craftmine': 161, 'invasion': 162, 'freedom': 163, 'month': 164, 'rest': 165, 'year': 166, 'change': 167, 'clock': 168, 'car': 169, 'read': 170, 'right': 171, 'time': 172, 'wait': 173, 'until': 174, 'daylight': 175, 'savings': 176, 'starts': 177, 'again': 178, 'd': 179, 'class': 180, 'image': 181, 'tagged': 182, 'drake': 183, 'hotline': 184, 'bling': 185, 'cave': 186, 'update': 187, 'fat': 188, 'fricking': 189, 'bees': 190, 'physical': 191, 'days': 192, 'look': 193, 'down': 194, 'friends': 195, 'common': 196, 'flu': 197, 'pepperoni': 198, 'woman': 199, 'anti': 200, 'anxiety': 201, 'medication': 202, 'adopt': 203, 'arsenal': 204, 'think': 205, \"i'm\": 206, 'nailing': 207, 'work': 208, 'says': 209, \"it's\": 210, 'keep': 211, 'up': 212, 'understanding': 213, 'why': 214, 'exclaims': 215, 'bread': 216, 'raw': 217, 'toast': 218, 'braces': 219, 'off': 220, 'years': 221, 'being': 222, 'quarantined': 223, 'because': 224, 'coronavirus': 225, 'all': 226, 'world': 227, 'roll': 228, 'lol': 229, 'visiting': 230, 'who': 231, 'official': 232, 'source': 233, 'whatsapp': 234, 'group': 235, 'youtube': 236, 'spotify': 237, 'tomoe': 238, 'having': 239, 'eternal': 240, 'mangekyo': 241, 'schools': 242, 'cancelled': 243, 'student': 244, 'runescape': 245, 'tunescape': 246, 'week': 247, 'special': 248, 'about': 249, 'generator': 250}\n",
      "Shape of X_images: (49, 7, 7, 512)\n",
      "Shape of X_captions: (49, 29)\n",
      "Shape of Y_captions: (49, 29, 251)\n",
      "Shape of Y_labels: (49, 65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to preprocess captions and labels\n",
    "def preprocess_caption(caption):\n",
    "    tokens = word_tokenize(caption.lower())\n",
    "    return tokens\n",
    "\n",
    "# Preprocess captions and build vocabulary\n",
    "captions = df['Captions'].apply(preprocess_caption).tolist()\n",
    "labels = df['Labels'].apply(lambda x: x.split(', ')).tolist()\n",
    "\n",
    "#  Display preprocessed captions and labels\n",
    "print(\"Preprocessed Captions:\")\n",
    "for caption in captions[:3]:  # Displaying the first 3 captions\n",
    "    print(caption)\n",
    "\n",
    "print(\"\\nLabels:\")\n",
    "for label in labels[:3]:  # Displaying the first 3 sets of labels\n",
    "    print(label)\n",
    "# Create tokenizers for captions and labels\n",
    "caption_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "caption_tokenizer.fit_on_texts(df[\"Captions\"])\n",
    "vocab_size = len(caption_tokenizer.word_index) + 1\n",
    "print(\"Tokenizer Vocabulary:\")\n",
    "print(caption_tokenizer.word_index)\n",
    "\n",
    "label_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "num_labels = len(label_tokenizer.word_index) + 1\n",
    "\n",
    "# Generate sequences from tokens\n",
    "caption_sequences = caption_tokenizer.texts_to_sequences(df[\"Captions\"])\n",
    "max_caption_length = max(len(seq) for seq in caption_sequences)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_caption_sequences = pad_sequences(caption_sequences, maxlen=max_caption_length, padding='post')\n",
    "\n",
    "# Prepare data for model training\n",
    "\n",
    "X_captions = np.array(padded_caption_sequences)\n",
    "Y_captions = to_categorical(X_captions, num_classes=vocab_size)\n",
    "\n",
    "# Convert labels to multi-hot encoded vectors\n",
    "Y_labels = label_tokenizer.texts_to_matrix(labels, mode='binary')\n",
    "\n",
    "\n",
    "print(f\"Shape of X_images: {X_images.shape}\")\n",
    "print(f\"Shape of X_captions: {X_captions.shape}\")\n",
    "print(f\"Shape of Y_captions: {Y_captions.shape}\")\n",
    "print(f\"Shape of Y_labels: {Y_labels.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deef30bc-fe5c-4104-bbf9-6e7cd63ca4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,100</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,422,784</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
       "│                               │                           │                 │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
       "│                               │                           │                 │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],              │\n",
       "│                               │                           │                 │ input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,968</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7279</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,870,703</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_13 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │          \u001b[38;5;34m25,100\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_2 (\u001b[38;5;33mNotEqual\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │       \u001b[38;5;34m6,422,784\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m365,568\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
       "│                               │                           │                 │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_15 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_2 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m577\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
       "│                               │                           │                 │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],              │\n",
       "│                               │                           │                 │ input_layer_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m147,968\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7279\u001b[0m)              │       \u001b[38;5;34m1,870,703\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m251\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,832,123</span> (33.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,832,123\u001b[0m (33.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,832,123</span> (33.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,832,123\u001b[0m (33.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 719ms/step - accuracy: 0.0221 - loss: 6.3652 - val_accuracy: 0.4655 - val_loss: 3.2860\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - accuracy: 0.3905 - loss: 3.4439 - val_accuracy: 0.7793 - val_loss: 2.4489\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - accuracy: 0.7310 - loss: 1.5770 - val_accuracy: 0.7759 - val_loss: 2.4098\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - accuracy: 0.8115 - loss: 0.8913 - val_accuracy: 0.7586 - val_loss: 2.3475\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 0.8710 - loss: 0.5299 - val_accuracy: 0.7379 - val_loss: 2.3539\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - accuracy: 0.9366 - loss: 0.3160 - val_accuracy: 0.7414 - val_loss: 2.4042\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - accuracy: 0.9745 - loss: 0.1761 - val_accuracy: 0.7517 - val_loss: 2.4663\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - accuracy: 0.9878 - loss: 0.1173 - val_accuracy: 0.7552 - val_loss: 2.5392\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - accuracy: 0.9947 - loss: 0.0953 - val_accuracy: 0.7552 - val_loss: 2.6139\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312ms/step - accuracy: 0.9947 - loss: 0.0893 - val_accuracy: 0.7517 - val_loss: 2.6861\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7517 - loss: 2.6861\n",
      "Test Accuracy: 75.17%\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "image_input = Input(shape=(25088,))\n",
    "caption_input = Input(shape=(max_caption_length,))\n",
    "label_input = Input(shape=(num_labels,))\n",
    "\n",
    "image_features = Dense(256, activation='relu')(image_input)\n",
    "caption_embedding = Embedding(vocab_size, 100, mask_zero=True)(caption_input)\n",
    "caption_lstm = LSTM(256)(caption_embedding)\n",
    "\n",
    "merged = Concatenate()([image_features, caption_lstm, label_input])\n",
    "decoder = Dense(256, activation='relu')(merged)\n",
    "output_caption = Dense(max_caption_length * vocab_size, activation='softmax')(decoder)  \n",
    "\n",
    "output_caption = Reshape((max_caption_length, vocab_size))(output_caption)  \n",
    "\n",
    "caption_model = Model(inputs=[image_input, caption_input, label_input], outputs=output_caption)\n",
    "\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "print(caption_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "X_images_train, X_images_test, X_captions_train, X_captions_test, Y_labels_train, Y_labels_test, urls_train, urls_test, Y_captions_train, Y_captions_test = train_test_split(\n",
    "    X_images, X_captions, Y_labels, df['Image URL'], Y_captions, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model \n",
    "history = caption_model.fit(\n",
    "    [X_images_train, X_captions_train, Y_labels_train], Y_captions_train,\n",
    "    validation_data=([X_images_test, X_captions_test, Y_labels_test], Y_captions_test),\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = caption_model.evaluate([X_images_test, X_captions_test, Y_labels_test], Y_captions_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f47aa88-3132-430c-8255-e466710115a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "['money', 'pepperoni', '', '', '', '', '', '', '', '', '', '', '', 'family', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Predicted Caption: money spotify                           \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/gtj5t.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: minecraft the freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/govs4.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: money pepperoni paper                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/tau4.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: minecraft the freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/25w3.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: money father freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/28s2gu.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: money father person who  source for  of my father   family    news minor          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/c2qn.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: food life freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/271ps6.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: money spotify freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/8h0c8.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: food life freedom                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/1bh9.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Predicted Caption: money pepperoni            family               \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgflip.com/1bhw.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "\n",
    "predicted_captions = caption_model.predict([X_images_test, X_captions_test, Y_labels_test])\n",
    "\n",
    "decoded_captions = []\n",
    "\n",
    "\n",
    "for prediction in predicted_captions:\n",
    "    \n",
    "    predicted_words = []\n",
    "    \n",
    "    \n",
    "    for timestep in prediction:\n",
    "        \n",
    "        predicted_word_index = np.argmax(timestep)\n",
    "        \n",
    "       \n",
    "        predicted_word = caption_tokenizer.index_word.get(predicted_word_index, '')  \n",
    "        \n",
    "       \n",
    "        predicted_words.append(predicted_word)\n",
    "    \n",
    "  \n",
    "    decoded_caption = ' '.join(predicted_words)\n",
    "    \n",
    "   \n",
    "    decoded_captions.append(decoded_caption)\n",
    "print(predicted_words)\n",
    "img_url=[]\n",
    "for url in urls_test:\n",
    "    img_url.append(url)\n",
    "    \n",
    "for i, caption in enumerate(decoded_captions):\n",
    "    print(f\"Predicted Caption: {caption}\")\n",
    "    display(IPImage(url=img_url[i]))\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472b0a7-6afa-4987-8e12-f22a8d8cdac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef82b45-bb52-416a-8a1c-b645dec05607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
